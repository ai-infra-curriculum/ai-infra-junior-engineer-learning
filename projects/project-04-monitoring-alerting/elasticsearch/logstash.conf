# Logstash Pipeline Configuration
#
# This file defines how Logstash processes logs:
# 1. Input: Receive logs from Filebeat
# 2. Filter: Parse, transform, enrich logs
# 3. Output: Send to Elasticsearch
#
# Learning Objectives:
# - Understand Logstash pipeline structure
# - Learn log parsing with grok and json filters
# - Implement log enrichment and transformation
# - Configure Elasticsearch output
#
# References:
# - https://www.elastic.co/guide/en/logstash/current/configuration.html
# - https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html

# =============================================================================
# INPUT: Receive Logs
# =============================================================================

input {
  # TODO: Configure Beats input to receive logs from Filebeat
  #
  # beats {
  #   port => 5044
  #   host => "0.0.0.0"
  #   client_inactivity_timeout => 3600
  # }
  #
  # Parameters:
  # - port: Port to listen on (default: 5044)
  # - host: Network interface to bind (0.0.0.0 = all interfaces)
  # - client_inactivity_timeout: Close idle connections after N seconds

  # TODO: (Optional) Add stdin input for testing
  # Useful for testing pipeline without Filebeat
  #
  # stdin {
  #   codec => json
  # }
}

# =============================================================================
# FILTER: Parse and Transform Logs
# =============================================================================

filter {
  # ---------------------------------------------------------------------------
  # Parse JSON Logs
  # ---------------------------------------------------------------------------

  # TODO: Parse JSON-formatted logs
  # Most modern applications log in JSON format
  #
  # if [message] =~ /^\{.*\}$/ {
  #   json {
  #     source => "message"
  #     target => "parsed"
  #     skip_on_invalid_json => true
  #   }
  # }
  #
  # This parses JSON from the 'message' field into 'parsed' object
  # Example input:
  # {"timestamp": "2025-10-18T10:00:00Z", "level": "INFO", "message": "Request completed"}
  #
  # After parsing:
  # parsed.timestamp = "2025-10-18T10:00:00Z"
  # parsed.level = "INFO"
  # parsed.message = "Request completed"


  # ---------------------------------------------------------------------------
  # Flatten Parsed JSON
  # ---------------------------------------------------------------------------

  # TODO: Move parsed fields to root level
  # Makes querying easier in Elasticsearch
  #
  # if [parsed] {
  #   mutate {
  #     rename => {
  #       "[parsed][timestamp]" => "@timestamp"
  #       "[parsed][level]" => "log_level"
  #       "[parsed][logger]" => "logger_name"
  #       "[parsed][message]" => "log_message"
  #       "[parsed][duration]" => "duration_ms"
  #     }
  #   }
  # }


  # ---------------------------------------------------------------------------
  # Parse Non-JSON Logs with Grok
  # ---------------------------------------------------------------------------

  # TODO: Parse unstructured logs using grok patterns
  # Grok uses regex patterns to extract fields
  #
  # else {
  #   grok {
  #     match => {
  #       "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:log_level}\] %{GREEDYDATA:log_message}"
  #     }
  #   }
  # }
  #
  # Grok patterns explained:
  # - %{TIMESTAMP_ISO8601:timestamp}: Matches ISO8601 timestamp, saves to 'timestamp'
  # - %{LOGLEVEL:log_level}: Matches log level (INFO, ERROR, etc.)
  # - %{GREEDYDATA:log_message}: Matches everything else
  #
  # Example input:
  # 2025-10-18T10:00:00.123Z [INFO] Request completed
  #
  # After parsing:
  # timestamp = "2025-10-18T10:00:00.123Z"
  # log_level = "INFO"
  # log_message = "Request completed"


  # ---------------------------------------------------------------------------
  # Date Parsing
  # ---------------------------------------------------------------------------

  # TODO: Parse timestamp field into @timestamp
  # Elasticsearch uses @timestamp for time-based indexing
  #
  # date {
  #   match => ["timestamp", "ISO8601"]
  #   target => "@timestamp"
  #   remove_field => ["timestamp"]
  # }
  #
  # This parses the 'timestamp' field and sets it as @timestamp


  # ---------------------------------------------------------------------------
  # Add Metadata
  # ---------------------------------------------------------------------------

  # TODO: Add useful metadata to logs
  #
  # mutate {
  #   add_field => {
  #     "environment" => "${ENVIRONMENT:development}"
  #     "service_name" => "${SERVICE_NAME:ml-api}"
  #   }
  # }
  #
  # Environment variables are useful for:
  # - Identifying which environment logs came from (dev/staging/prod)
  # - Service identification in multi-service deployments


  # ---------------------------------------------------------------------------
  # Parse Error Stack Traces
  # ---------------------------------------------------------------------------

  # TODO: Handle multi-line stack traces
  # Error stack traces often span multiple lines
  #
  # if [log_level] == "ERROR" and [message] =~ /Traceback/ {
  #   multiline {
  #     pattern => "^\s"
  #     what => "previous"
  #     negate => false
  #   }
  # }
  #
  # This combines lines starting with whitespace with the previous line


  # ---------------------------------------------------------------------------
  # Extract Structured Data from Log Message
  # ---------------------------------------------------------------------------

  # TODO: Extract specific fields from log messages using grok
  # Example: Extract request_id, endpoint, duration from message
  #
  # if [log_message] =~ /request_id/ {
  #   grok {
  #     match => {
  #       "log_message" => "request_id=%{UUID:request_id} endpoint=%{URIPATH:endpoint} duration=%{NUMBER:duration_ms:float}"
  #     }
  #   }
  # }
  #
  # Example message:
  # "request_id=abc-123-def endpoint=/predict duration=45.2"
  #
  # Extracts:
  # request_id = "abc-123-def"
  # endpoint = "/predict"
  # duration_ms = 45.2


  # ---------------------------------------------------------------------------
  # GeoIP Enrichment (Optional)
  # ---------------------------------------------------------------------------

  # TODO: Add geographic information from IP addresses
  # Useful for tracking where requests come from
  #
  # if [client_ip] {
  #   geoip {
  #     source => "client_ip"
  #     target => "geoip"
  #     database => "/usr/share/logstash/GeoLite2-City.mmdb"
  #   }
  # }
  #
  # Adds:
  # geoip.country_name = "United States"
  # geoip.city_name = "San Francisco"
  # geoip.location.lat = 37.7749
  # geoip.location.lon = -122.4194


  # ---------------------------------------------------------------------------
  # Remove Sensitive Data
  # ---------------------------------------------------------------------------

  # TODO: Redact sensitive information from logs
  # IMPORTANT: Never log passwords, API keys, credit cards, etc.
  #
  # mutate {
  #   gsub => [
  #     "log_message", "password=\S+", "password=[REDACTED]",
  #     "log_message", "api_key=\S+", "api_key=[REDACTED]",
  #     "log_message", "token=\S+", "token=[REDACTED]"
  #   ]
  # }
  #
  # This replaces sensitive values with [REDACTED]


  # ---------------------------------------------------------------------------
  # Convert Data Types
  # ---------------------------------------------------------------------------

  # TODO: Convert string fields to appropriate types
  # Elasticsearch works better with correct types
  #
  # mutate {
  #   convert => {
  #     "duration_ms" => "float"
  #     "status_code" => "integer"
  #     "is_error" => "boolean"
  #   }
  # }


  # ---------------------------------------------------------------------------
  # Add Tags for Easy Filtering
  # ---------------------------------------------------------------------------

  # TODO: Add tags based on log content
  # Tags make filtering easier in Kibana
  #
  # if [log_level] == "ERROR" {
  #   mutate {
  #     add_tag => ["error"]
  #   }
  # }
  #
  # if [endpoint] == "/predict" {
  #   mutate {
  #     add_tag => ["prediction", "ml"]
  #   }
  # }
  #
  # if [duration_ms] > 1000 {
  #   mutate {
  #     add_tag => ["slow_request"]
  #   }
  # }


  # ---------------------------------------------------------------------------
  # Calculate Derived Fields
  # ---------------------------------------------------------------------------

  # TODO: Add calculated fields
  #
  # ruby {
  #   code => '
  #     if event.get("duration_ms")
  #       event.set("duration_seconds", event.get("duration_ms") / 1000.0)
  #     end
  #   '
  # }


  # ---------------------------------------------------------------------------
  # Drop Unnecessary Fields
  # ---------------------------------------------------------------------------

  # TODO: Remove fields you don't need in Elasticsearch
  # Reduces storage and improves query performance
  #
  # mutate {
  #   remove_field => [
  #     "[beat][hostname]",
  #     "[beat][version]",
  #     "agent",
  #     "ecs",
  #     "input",
  #     "host"
  #   ]
  # }


  # ---------------------------------------------------------------------------
  # Handle Parsing Failures
  # ---------------------------------------------------------------------------

  # TODO: Tag documents that failed parsing
  # Allows you to find and debug parsing issues
  #
  # if "_grokparsefailure" in [tags] or "_jsonparsefailure" in [tags] {
  #   mutate {
  #     add_tag => ["parse_failure"]
  #     add_field => {
  #       "parse_error" => "Failed to parse log message"
  #     }
  #   }
  # }
}

# =============================================================================
# OUTPUT: Send to Elasticsearch
# =============================================================================

output {
  # TODO: Configure Elasticsearch output
  #
  # elasticsearch {
  #   hosts => ["elasticsearch:9200"]
  #   index => "logs-%{+YYYY.MM.dd}"
  #   document_type => "_doc"
  #   template_name => "logs"
  #   template_overwrite => true
  # }
  #
  # Parameters:
  # - hosts: Elasticsearch cluster nodes
  # - index: Index name pattern (creates daily indices)
  #   - logs-2025.10.18
  #   - logs-2025.10.17
  #   This makes it easy to delete old logs
  # - document_type: Always "_doc" in Elasticsearch 7+
  # - template_name: Custom index template (optional)


  # TODO: (Optional) Add stdout output for debugging
  # Useful during development to see processed logs
  #
  # stdout {
  #   codec => rubydebug
  # }
  #
  # This prints each processed log event to console


  # TODO: (Optional) Add conditional outputs based on log level
  # Example: Send errors to separate index
  #
  # if [log_level] == "ERROR" {
  #   elasticsearch {
  #     hosts => ["elasticsearch:9200"]
  #     index => "errors-%{+YYYY.MM.dd}"
  #   }
  # }


  # TODO: (Optional) Send critical errors to S3 for archival
  #
  # if [log_level] == "CRITICAL" {
  #   s3 {
  #     access_key_id => "${AWS_ACCESS_KEY_ID}"
  #     secret_access_key => "${AWS_SECRET_ACCESS_KEY}"
  #     region => "us-west-2"
  #     bucket => "critical-logs"
  #     prefix => "logs/%{+YYYY}/%{+MM}/%{+dd}/"
  #     codec => "json_lines"
  #   }
  # }
}

# =============================================================================
# Testing Your Pipeline
# =============================================================================

# Method 1: Test with stdin/stdout
# 1. Comment out beats input, elasticsearch output
# 2. Uncomment stdin input, stdout output
# 3. Run: docker-compose exec logstash /usr/share/logstash/bin/logstash -f /usr/share/logstash/pipeline/logstash.conf
# 4. Type JSON: {"timestamp": "2025-10-18T10:00:00Z", "level": "INFO", "message": "Test"}
# 5. See parsed output

# Method 2: Check processed logs in Elasticsearch
# curl http://localhost:9200/logs-*/_search?pretty

# Method 3: View in Kibana
# http://localhost:5601 → Discover → Select index pattern "logs-*"


# =============================================================================
# Common Grok Patterns
# =============================================================================

# %{TIMESTAMP_ISO8601}     # 2025-10-18T14:32:10.123Z
# %{LOGLEVEL}              # INFO, ERROR, WARNING, DEBUG
# %{NUMBER}                # 123, 45.67
# %{UUID}                  # 123e4567-e89b-12d3-a456-426614174000
# %{IP}                    # 192.168.1.1
# %{URIPATH}               # /api/v1/predict
# %{GREEDYDATA}            # Matches everything (use at end)
# %{WORD}                  # Single word
# %{QUOTEDSTRING}          # "quoted string"

# Custom pattern:
# %{PATTERN:field_name:data_type}
# Example: %{NUMBER:duration:float}


# =============================================================================
# Performance Tips
# =============================================================================

# 1. Use JSON logging whenever possible
#    - Faster than grok parsing
#    - More reliable
#    - Easier to work with

# 2. Minimize grok patterns
#    - Grok is CPU-intensive
#    - Use specific patterns, avoid GREEDYDATA

# 3. Filter early
#    - Drop unnecessary logs before processing
#    - Reduces load on Elasticsearch

# 4. Batch processing
#    - Logstash batches events for efficiency
#    - Default: 125 events per batch

# 5. Multiple pipelines
#    - Separate pipelines for different log types
#    - Prevents one slow source from blocking others
