# Lecture 01: Observability Fundamentals for AI Infrastructure

## Lecture Overview
Monitoring and observability are indispensable for running production-grade AI infrastructure. This lecture establishes the conceptual foundation you will rely on throughout the module: key terminology, mental models, service level objectives (SLOs), observability signals, and the unique challenges involved in operating ML workloads. By the end, you will possess a practical checklist for instrumenting systems and a roadmap for elevating observability maturity inside your organization.

**Estimated Reading Time:** 60–75 minutes  
**Hands-on Companion Lab:** Exercise 01 – Observability Foundations Lab  
**Prerequisite Knowledge:** Linux command line, Docker basics, familiarity with REST APIs, introductory AI/ML pipeline concepts.

---

## 1. Setting the Stage

### 1.1 Monitoring vs Observability
- **Monitoring** collects predefined metrics to detect failures that you anticipate. It answers the question: “Is my system behaving within expected bounds?”
- **Observability** measures how well you can understand the internal state of a system from the outside by examining its outputs (metrics, logs, traces, profiles, events). An observable system lets you debug the unknown-unknowns.

> **Rule of thumb:** Monitoring informs you when thresholds are breached; observability equips you to explain *why*.

### 1.2 Why Observability Matters for AI Infrastructure
AI systems are probabilistic, data-dependent, and often involve distributed training/inference pipelines. Failures can hide in:
- Data quality issues (e.g., schema drift, missing features).
- Hardware bottlenecks (GPU saturation, PCIe errors).
- Model behaviour (accuracy degradation, bias drift).
- Operational workflows (stuck jobs, backpressure, cost blow-outs).

Traditional server metrics alone cannot capture these states. Observability enables:
- Rapid incident response and minimization of Mean Time to Repair (MTTR).
- Compliance with regulatory and ethical guidelines (audit trails, fairness checks).
- Capacity planning and cost attribution (GPU hours, cloud spend).
- Continuous improvement loops between data scientists, MLOps, and infrastructure teams.

### 1.3 An Illustrative Failure
Consider an inference service that suddenly doubles its tail latency. Without observability, operators only see 500 errors. With observability you uncover:
1. **Metrics:** GPU utilization at 98%, increased queue depth.
2. **Logs:** Frequent warnings about batch size falling back to 1.
3. **Traces:** A new upstream feature extraction microservice adds 300 ms.
4. **Model signals:** Request distribution shifted geographically causing heavier pre-processing.

Only by correlating these signals can you identify the root cause (misconfigured rollout that disabled caching). This lecture teaches the building blocks that make the above diagnosis possible.

---

## 2. Core Concepts and Terminology

### 2.1 The Three Pillars of Observability
1. **Metrics** – Numerical measurements collected over time (e.g., request latency, GPU temperature).
2. **Logs** – Discrete event records (text or structured) capturing contextual information.
3. **Traces** – End-to-end request flows across services, showing latency breakdowns.

Many teams add two more dimensions:
- **Events** (deploys, feature flags, business outcomes).
- **Profiles** (CPU/memory/latency flame graphs).

### 2.2 Golden Signals and Beyond
Google’s SRE book popularized the **Four Golden Signals**:
- **Latency** – Time to serve a request.
- **Traffic** – Volume of requests/operations.
- **Errors** – Rate of failed requests.
- **Saturation** – Utilization of constrained resources.

For AI/ML workloads, expand with:
- **Model Quality** – Accuracy, precision/recall, confusion matrices.
- **Data Quality** – Freshness, completeness, drift, feature null ratios.
- **Resource Efficiency** – GPU hours, cost per 1,000 predictions, energy usage.

### 2.3 Service Level Objectives (SLO), SLIs, and SLAs
- **Service Level Indicator (SLI):** Quantifiable measure of service performance (e.g., “Percentage of inference requests under 200 ms latency”).
- **Service Level Objective (SLO):** Target value/range for an SLI (e.g., “99% of inferences must complete under 200 ms over a rolling 30-day window”).
- **Service Level Agreement (SLA):** External contract with consequences if SLOs are missed (e.g., financial penalties, crediting customers).

**Error budget:** 100% − SLO. Represents how much unreliability you can afford. Guides decision-making (deploy velocity vs stability).

### 2.4 Observability Maturity Model
| Level | Characteristics | What’s Missing |
|-------|----------------|----------------|
| Level 0 – Reactive Monitoring | Manual checks, scripts, no dashboards | No alerting, no standard metrics |
| Level 1 – Instrumented | Metrics/logs fleetwide, dashboards available | Traces limited, SLOs undefined |
| Level 2 – Proactive Observability | SLO-driven dashboards, anomaly detection | Weak ML-specific instrumentation |
| Level 3 – Predictive & Adaptive | Auto-remediation, closed feedback loops, business-aware | Ongoing investment in automation |

Assess where your organization sits to plan improvements.

---

## 3. Building Observability for AI Systems

### 3.1 Layered Monitoring Strategy
1. **Infrastructure Layer:** Physical/virtual hosts, containers, Kubernetes clusters, storage, networking.
2. **Platform Layer:** Feature stores, orchestration systems, experiment tracking, data pipelines.
3. **Application Layer:** REST/gRPC inference services, training jobs, batch jobs.
4. **Business Layer:** Customer impact (conversion rate, number of active users), compliance metrics.

Each layer should expose relevant SLIs, with clear ownership.

### 3.2 Instrumentation Principles
- **Standardize metric names** using a consistent prefix (e.g., `ai_infra_training_duration_seconds`).
- **Label responsibly:** Avoid high-cardinality labels (e.g., per-user IDs).
- **Automate instrumentation**: Middleware for HTTP metrics, wrappers around GPU libraries, logging middleware for message queues.
- **Correlate signals:** Include trace IDs in logs, attach request IDs to metrics, embed experiment identifiers.
- **Version schemas:** Use structured logging (JSON) to ensure downstream tools can parse fields reliably.

### 3.3 Observability in the ML Lifecycle
- **Data Ingestion:** Monitor data freshness, schema validations (Great Expectations), pipeline SLIs.
- **Training:** Track job duration, cost, GPU utilization, gradient norms, checkpoint size.
- **Evaluation:** Capture metrics per dataset slice, fairness metrics (e.g., demographic parity).
- **Deployment:** Monitor rollout progress, canary comparison, shadow deployment metrics.
- **Serving:** Track real-time accuracy (if ground truth available), request latency, drift detectors.

### 3.4 Case Study: Feature Store Outage
**Scenario:** Feature store latency spike causes inference timeouts.

**Signals observed:**
- `feature_store_latency_seconds_bucket` histogram trending upward.
- gRPC client logs show retries/backoff.
- Trace spans reveal 80% of latency spent in feature store call.
- Data pipeline metrics indicate a backlog due to upstream schema change.

**Resolution plan:**
1. Triggered alert on high P95 latency.
2. Incident commander correlates metrics and logs.
3. Temporary mitigation using fallback features.
4. Long-term fix: schema validation guard + SLO revision.

Use this as a template for building observability playbooks.

---

## 4. Designing Effective Dashboards

### 4.1 Principles
- **Purpose-driven:** Each dashboard answers a specific question (e.g., on-call triage vs executive overview).
- **Hierarchy:** Start with high-level health, drill down to component-level panels.
- **Contextual clues:** Include annotations for deploys, feature toggles, incidents.
- **Consistency:** Apply common color schemes, axis scales, naming conventions.
- **Discoverability:** Document dashboards in README or runbooks.

### 4.2 Dashboard Types for AI Infrastructure
1. **Platform Health Dashboard**
   - Kubernetes node health, control plane metrics.
   - GPU utilization & errors (exported via DCGM exporter).
   - Queue lengths (Kafka, RabbitMQ).
2. **Model Performance Dashboard**
   - Real-time accuracy vs target, drift metrics.
   - Feature distribution plots (overlays vs training baseline).
   - SLO heatmap across regions/customers.
3. **Data Pipeline Dashboard**
   - Airflow/Kubeflow task duration, success rate.
   - Data freshness lags, row counts, schema validation failures.
4. **Business Impact Dashboard**
   - Uptime vs SLA, user conversions, cost per prediction.

### 4.3 Dashboard Anti-Patterns
- Overloaded panels with multiple Y-axes and inconsistent scales.
- Raw counts without normalization (difficult cross-environment comparison).
- Too many red/yellow colors causing alarm fatigue.
- Lack of alert links or runbook references.

---

## 5. Logging Strategies

### 5.1 Structured Logging
Replace free-form strings with structured JSON logs:
```json
{
  "timestamp": "2025-10-18T19:02:14.321Z",
  "service": "inference-gateway",
  "level": "ERROR",
  "event": "batch_inference_failed",
  "model_version": "recommendation_v2:2025-10-12",
  "trace_id": "c7509a0d85cbee42",
  "latency_ms": 347,
  "error": {
    "type": "FeatureNotFound",
    "missing_feature": "user_last_purchase"
  }
}
```
Benefits:
- Easily parsed by log aggregators (Loki, Elastic, Splunk).
- Facilitates correlation with traces and metrics.
- Supports compliance (PII masking, retention policies).

### 5.2 Logging Levels and Policies
- Establish guidelines for each level (DEBUG, INFO, WARN, ERROR, FATAL).
- Keep DEBUG logs out of production unless toggled.
- Scrub sensitive data (PII, secrets) before logging.
- Define retention periods (e.g., 30 days for INFO, 180 days for compliance logs).

### 5.3 Log Aggregation Patterns
- **Agent-based:** Fluent Bit, Vector, Filebeat tail local files and forward to aggregator.
- **Sidecar pattern:** Each pod has a sidecar shipping logs; common in Kubernetes.
- **Direct app integration:** Applications send logs over network (structured log sinks).
- Evaluate trade-offs (resource overhead, reliability, security).

---

## 6. Alerting Foundations

### 6.1 Good Alert Characteristics
- **Actionable:** Clear remediation steps exist.
- **Impactful:** Alerts correspond to user-facing or business-critical SLIs.
- **Urgent:** Requires timely response within on-call hours.
- **Unique:** Avoid multiple alerts for same underlying issue; deduplicate through Alertmanager.

### 6.2 Alert Design Process
1. Select SLI/SLO relevant to service or ML workflow.
2. Define thresholds and durations (e.g., P99 latency > 1.5 seconds for 10 minutes).
3. Contextualize alert message:
   - “Latency SLO violation: inference-gateway P99=1.8s (>1.5s) for 15m. Error budget burn rate 4x. See runbook: go/runbook-inference-latency.”
4. Route to correct on-call rotation or escalation chain.
5. Set up alert suppression during maintenance windows or known safe anomalies (e.g., training jobs nightly).

### 6.3 Burn Rate Alerts
- Use multi-window, multi-burn-rate alerts to detect SLO violations early without noise. Example:
  - Short window: 5-minute burn rate > 14 (fast detection).
  - Long window: 1-hour burn rate > 4 (avoids flapping).

---

## 7. Observability for Machine Learning Systems

### 7.1 Monitoring Model Quality
- **Online metrics:** Real-time accuracy when ground truth is available quickly.
- **Proxy metrics:** User engagement, click-through rate, or business KPIs.
- **Delayed metrics:** Offline evaluation from periodic labeling or human review.
- **Fairness metrics:** Track disparate impact across cohorts.

### 7.2 Data Drift and Concept Drift Detection
- Monitor feature distributions vs training baseline using statistical tests (KL divergence, PSI).
- Leverage tools like Evidently, WhyLabs, Arize.
- Alert when drift persists, not just single-sample anomalies.

### 7.3 Model Explainability and Auditing
- Log feature importance, SHAP values, or counterfactual explanations.
- Retain data for reproducing predictions (within compliance boundaries).
- Monitor compliance: authorized access, PII exposure, GDPR/CCPA requests.

### 7.4 Feedback Loops
- Close the loop by feeding post-deployment metrics back to training teams.
- Adopt runbooks for “model performance degradation” similar to infrastructure incidents.

---

## 8. Tooling Landscape

| Category | Open Source | Managed / Commercial |
|----------|-------------|----------------------|
| Metrics + TSDB | Prometheus, VictoriaMetrics, Mimir | Datadog, New Relic, Cloud Monitoring |
| Visualization | Grafana | Datadog, New Relic, Chronosphere |
| Logging | Loki, Elastic Stack, OpenSearch | Splunk, Datadog Logs, Sumo Logic |
| Tracing | Jaeger, Tempo, OpenTelemetry Collector | Honeycomb, Lightstep, AWS X-Ray |
| ML Observability | Evidently, Feast + Grafana | Arize AI, WhyLabs, Mona, Fiddler |

**Integration Strategy:** 
- Start with Prometheus + Grafana + Loki (aka PLG stack) for unified telemetry.
- Layer OpenTelemetry for standardized instrumentation.
- Leverage managed offerings when team size is small or compliance requires vendor support.

---

## 9. Practical Checklist

### 9.1 Readiness Checklist
- [ ] Metrics coverage for infrastructure, platform, application, business layers.
- [ ] SLIs defined with SLO targets and tracked on dashboards.
- [ ] Logs structured, centralized, and correlated with request IDs.
- [ ] Trace propagation implemented across HTTP/gRPC boundaries.
- [ ] Alert thresholds, routing, and escalation policies documented.
- [ ] Runbooks exist for top 5 failure modes (e.g., GPU starvation, data drift).
- [ ] Observability data retained according to compliance rules.
- [ ] Cost of observability tooling monitored (storage, query costs).

### 9.2 Implementation Steps
1. Inventory current instrumentation; identify gaps vs golden signals.
2. Prioritize high-impact SLIs (user-facing and compliance-driven).
3. Standardize logging and metric schemas across teams.
4. Automate dashboard provisioning via infrastructure-as-code (e.g., Grafana provisioning, Terraform).
5. Regularly review alerts to prevent fatigue; run monthly observability retro.

### 9.3 Collaboration Tips
- Embed observability requirements in Definition of Done (DoD) for new services.
- Pair data scientists with platform engineers to define ML-specific SLIs.
- Share observability insights with leadership (trend reports, maturity assessments).
- Schedule cross-team “observability game days” to rehearse incident response.

---

## 10. Knowledge Check

Test your understanding with these reflection questions (answers at end of module):
1. Differentiate monitoring from observability using an example from ML operations.
2. Define three SLIs for a batch inference pipeline and propose SLO targets.
3. Describe how you would detect and investigate a sudden rise in model false positives.
4. What steps would you take to reduce alert fatigue while maintaining coverage?
5. How can you ensure logs, metrics, and traces are correlated during an incident?

---

## 11. Further Reading & Resources
- Google SRE Workbook – Chapters on alerting and SLOs.
- “Observability Engineering” by Charity Majors, Liz Fong-Jones, George Miranda.
- OpenTelemetry documentation: instrumentation guides for Python, Go, Java.
- Arize AI blog: ML observability use cases and patterns.
- Grafana Labs learning portal: dashboards and alerting tutorials.
- WhyLabs Academy: data drift monitoring.

**Hands-on practice:** Complete Exercise 01 to instrument a sample service using the checklists introduced here.

---

## 12. Summary
- Observability enables you to explain system behaviour, not just detect anomalies.
- AI infrastructure demands expanded observability covering data, model quality, and cost.
- Adopt SLO-driven observability with layered metrics, structured logging, and trace correlation.
- Build purposeful dashboards, actionable alerts, and comprehensive runbooks.
- Treat observability as an iterative journey—continually refine instrumentation, collaboration, and tooling.

In the next lecture, we will dive into Prometheus architecture, PromQL, and instrumentation techniques to realize the metrics pillar in practice.
